0.  How much time did you spend on this pre-class exercise, and when?
	45 minutes on 10/6

1.  What are one or two points that you found least clear in the
    10/08 slide decks (including the narration)?
	
	I'm still a bit confused on how the threads are assigned. 
	ex from slide 17: http://cornell-cs5220-f15.github.io/slides/2015-10-08-mpi.html#/17
	there is an MPI_broadcast and then a function call (run_ntrial)
	are we running the main function on each processor separately? or only on the main node and somehow MPI figures out that each processor should call the run_trial function?
	
	
	
	
2.  Now that we are now basically a third of the way into the
    semester, and are (mostly) settled into the steady pace of things,
    I would appreciate your feedback on what is working well or poorly
    about the class.  Comments on things I can reasonably change are
    particularly useful -- venting about the cluster, for example, is
    understandable but doesn't help me that much in adjusting!
	
	When you introduce subjects (especially if they were not in the slides) it would good if you could give a quick explanation (or a reminder) of
	the basics of what we are talking about and what they are used for. 
	I find that I sometimes get lost very quickly because I have no idea what a term that is probably obvious to most of the class means.
	For example last Thursday you discussed different kind of offloads,	and I did not know what offloads were or were used for so I didn't understand much of that section of the class.
	
	The projects are very interesting, but a bit overwhelming as they require a very large amount of time to do really well,
	especially for someone who is not comfortable with a lot of the tools we are using in the class.
	Also, the randomly assigned groups seems to create an incentive for some people to not do anything, which does not help with the assignment being overwhelming.
	
	
	
	
	
3.  The ring demo implements the protocol described in the particle
    systems slide deck from 9/15:

    http://cornell-cs5220-f15.github.io/slides/2015-09-15-particle.html#/11

    a) In your own words, describe what ring.c is doing.
	creates a local array which is part of the total domain, then computes interaction within the domain, then 
	passes on the data to the next processor, and waits for the data from the previous processor.
	Computes interaction with data that was given, and then passes send/receive again and repeat.
	
    b) How might you modify the code to have the same computational
       pattern, but using non-blocking communication rather than
       MPI_Sendrecv?  Note that according to the MPI standard,
       one isn't supposed to read from a buffer that is being
       handled by a non-blocking send, so it is probably necessary
       to use three temporary buffers rather than the current two.
		
		Isend (current_buf)
			receive (recv_buff)
			new_buff = copy of current_buf
			$$$ compute interactions
		Iwait
		current_buff=new_buff
		
		
		